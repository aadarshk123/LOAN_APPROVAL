# -*- coding: utf-8 -*-
"""ML_PROJECT_LOAN_APPROVAL__.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CrKG5xNq4wX90Ix27w_V9UDsd5b8kS-J

# **2.Home Loan Approval**
**Source** : [https://www.kaggle.com/datasets/rishikeshkonapure/home-loan-approval](https://www.kaggle.com/datasets/rishikeshkonapure/home-loan-approval)

**DESCRIPTION** : There are 12 attributes(Columns) and 1 classification attributes having 2 classes((Y=1), (N=0)) and 614 records/instances.

**From the above dataset we predict the Loan-Status of a Person**

## **Attributes** :

**Loan_ID** - Unique Id for particular applied Loan

**Gender** - Applicant's Gender

**Married** -  Applicant's marital status

**Dependents** - Number of dependents

**Education** - Applicant's qualification

**self_Employed** - Applicant's Employeed or not

**ApplicantIncome** - Applicant's income

**CoapplicantIncome** - Co-Applicant's	

**LoanAmount** - Amount for Loan

**Loan_Amount_Term** - Loan-Amount Term	

**Credit_History** - Applicant's credit-History

**Property_Area** - Applicant's Property-Area

**Loan_Status** - Applicant's Loan Status(Y/N)

# **3. Exploratory Data Analysis**
"""

#Importing and printing dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df=pd.read_csv("loan_sanction.csv")
df

#Columns
print("The Columns are : ")
df.columns

#Descibing the dataset
df.describe()

df.info()

#Dimensionality of dataset
print("Dimension of the dataset :",df.ndim)
print("Shape of the dataset :",df.shape)

#Numerical Features
numerical_features = df.select_dtypes(include=['float','int']).columns.tolist()
numerical_features

#Categorical Features
categorical_features = df.select_dtypes(exclude=['float','int']).columns.tolist()
categorical_features

#Performing Label Encoding
from sklearn.preprocessing import LabelEncoder
Loan_ID_label=LabelEncoder()
Gender_label=LabelEncoder()
Married_label=LabelEncoder()
Dependents_label=LabelEncoder()
Education_label=LabelEncoder()
Self_Employed_label=LabelEncoder()
#Property_Area_label=LabelEncoder()

df=pd.get_dummies(df, columns = ['Property_Area'])
Loan_Status_label=LabelEncoder()

print(df)

# df['Loan_ID']=Loan_ID_label.fit_transform(df['Loan_ID'])
df['Gender']=Gender_label.fit_transform(df['Gender'])
df['Married']=Married_label.fit_transform(df['Married'])
df['Dependents']=Dependents_label.fit_transform(df['Dependents'])
df['Education']=Education_label.fit_transform(df['Education'])
df['Self_Employed']=Self_Employed_label.fit_transform(df['Self_Employed'])
#df['Property_Area']=Property_Area_label.fit_transform(df['Property_Area'])
df['Loan_Status']=Loan_Status_label.fit_transform(df['Loan_Status'])
df

#Data-Types after Encoding
df.dtypes

#Computing Null Values
df.isnull().sum()

#Removing Null values for continuous variable by MEDIAN stratagy
from sklearn.impute import SimpleImputer

#IDENTIFYING AND FILLING NUMERICAL VALUES BY THE MEDIAN
num_imputer = SimpleImputer(strategy='median')

num_imputer.fit(df[numerical_features])
SimpleImputer(strategy='median')

df[numerical_features]=num_imputer.transform(df[numerical_features])


# #IDENTIFYING AND FILLING CATEGORICAL VALUES BY THE CONSTANT
# cat_imputer = SimpleImputer(strategy = 'constant', fill_value='missing')
# cat_imputer.fit(df[categorical_features])
# SimpleImputer(fill_value='missing', strategy='constant')

# df[categorical_features] = cat_imputer.transform(df[categorical_features])
# df

#After removing Null Values , the Null Counts
df.isnull().sum()

"""# **4. Pre-processsing**"""

#Histogram plot
fig, ax = plt.subplots(figsize=(25,14), dpi=50);
df.hist(ax=ax, layout=(5,3), alpha=0.5);

#Pair plot
#sns.pairplot(df)

#Pearson's Matrix
import plotly.express as px

# df = px.data.medals_wide(indexed=True)
fig = px.imshow(df.corr(),text_auto=True,aspect="auto")
fig.show()

#Distinguishing featured(Independent) and dependent attributes
print(abs(df.corr()["Loan_Status"]).nlargest(10).index[1:])
features = df[df.corr()["Loan_Status"].nlargest(10).index[:10]]
dependent = df.iloc[:, -1]

#Featured attributes
features

#Dependent Variables
dependent

#Identifying the Outliers using Boxplot
import plotly.express as px
import plotly.graph_objects as go
fig = go.Figure()

for col in df:
  fig.add_trace(go.Box(y=df[col].values, name=df[col].name))
  
fig.show()

"""# **Featured Scaling**"""

#Performing StandardScaler
from sklearn.preprocessing import MinMaxScaler, StandardScaler

def normalize(X):
    print("\nMean and Standard Deviation Before")
    print(X.mean(axis=0), X.std(axis=0))
        
    sc=StandardScaler()
    XScaled = sc.fit_transform(X)
        
    print("\nMean and Standard Deviation After")
    print(XScaled.mean(axis=0).round(4), XScaled.std(axis=0))
    return XScaled

"""# **Logistic Regression**"""

from sklearn.metrics import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold
from sklearn .metrics import roc_auc_score

def logisticRegresssion():    
    X = features.values
    # print(X)
    Y = dependent.values
    # print(Y)
    # print(type(Y))
    # Check for NaN values and replace them with mean value of the corresponding column
    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
    X = imp_mean.fit_transform(X)
    
    print("*************Normalizing the Featured/Independent Valuess*************")
    XScaled = normalize(X) 

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)
    acc = []
    y_ori = np.array([], dtype=int)
    y_pre = np.array([], dtype=int)
    net_mat = np.zeros((2,2))
    roc = []
    
    for train_index, test_index in skf.split(X,Y):
        X_train = X[train_index]
        X_test = X[test_index]
        Y_train = Y[train_index]
        Y_test = Y[test_index]
        print("\n") 
        print(f"Shape of X_train - {X_train.shape}  AND  Shape of X_test - {X_test.shape}")   
        print(f"Shape of Y_train - {Y_train.shape}  AND  Shape of Y_test - {Y_test.shape}")
        # print(f"For {i+1} fold : ")      
        print(f"\nTrain - {np.bincount(Y[train_index])}   AND  Test - {np.bincount(Y[test_index])} \n")  
        
        LRModel = LogisticRegression(max_iter=1000000)
        LRModel.fit(X_train, Y_train)
        Y_testPred = LRModel.predict(X_test)
        y_ori = np.hstack((y_ori,Y_test))
        y_pre = np.hstack((y_pre,Y_testPred))   
        testAccuracy = metrics.accuracy_score(Y_test, Y_testPred)
        print("Test Accuracy : ", testAccuracy*100)
        acc.append(testAccuracy)

        roc_score = roc_auc_score(Y_test, LRModel.predict_proba(X_test)[:, 1], multi_class='ovr')
        print("ROC_AUC_SCORE : ",roc_score)  

        matrix1 = confusion_matrix(Y_test, Y_testPred)
        net_mat = net_mat+matrix1
        plot_confusion_matrix(matrix1,show_normed=True, colorbar=True, show_absolute=True)   
        plt.show()
        
    print("Result Of Logistic Regression: \n")
    avg_accuracy = (sum(acc) / len(acc))*100
    print("Average accuracy : ", avg_accuracy)        
    net_mat = net_mat.astype('int')
    print(net_mat)
    plot_confusion_matrix(net_mat, show_normed=True, colorbar=True, show_absolute=True, cmap='Blues')   
    plt.show()
    # print(X_test.shape,Y_test.shape)
    # print(LRModel.predict_proba(X_test))
    # roc_score = roc_auc_score(Y_test, LRModel.predict_proba(X_test), multi_class='ovr')
    # roc_score = roc_auc_score(Y_test.ravel(), LRModel.predict_proba(X_test), multi_class='ovr')
    roc_score = roc_auc_score(Y_test, LRModel.predict_proba(X_test)[:, 1], multi_class='ovo')

    print("ROC_AUC_SCORE : ",roc_score)
    print("\nClassification Report : \n")
    report = classification_report(y_ori, y_pre, output_dict=True)
    report_Df = pd.DataFrame(report)
    print(report_Df)
    sns.heatmap(report_Df.T,annot=True)
    avg_accuracy

logisticRegresssion()

"""# **Decision Tree Classifier**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.tree import export_graphviz
from six import StringIO  
from IPython.display import Image  
import pydotplus

def decisionTree():
  X=features.values
  Y=dependent.values
  # Check for NaN values and replace them with mean value of the corresponding column
  imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
  X = imp_mean.fit_transform(X)
  print("*************Normalizing the Featured/Independent Valuess*************")
  XScaled = normalize(X) 

  skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2)
  acc=[]

  y_ori = np.array([], dtype=int)

  y_pre= np.array([], dtype=int)
  net_mat=np.zeros((2,2))
  
  for train_index, test_index in skf.split(X,Y):
      
      X_train=X[train_index]
      X_test=X[test_index]
      Y_train=Y[train_index]
      Y_test=Y[test_index]

      print("\n") 
      print(f"Shape of X_train - {X_train.shape}  AND  Shape of X_test - {X_test.shape}")   
      print(f"Shape of Y_train - {Y_train.shape}  AND  Shape of Y_test - {Y_test.shape}")
      # print(f"For {i+1} fold : ")      
      print(f"\nTrain - {np.bincount(Y[train_index])}   AND  Test - {np.bincount(Y[test_index])} \n")

      treemodel=DecisionTreeClassifier(criterion='entropy',max_depth=4)
      treemodel.fit(X_train, Y_train)

      Y_testPred =treemodel.predict(X_test)

      y_ori=np.hstack((y_ori,Y_test))
      
  
      y_pre=np.hstack((y_pre,Y_testPred))   
      testAccuracy = metrics.accuracy_score(Y_test, Y_testPred)
      print("Test Accuracy : ", testAccuracy*100)
      acc.append(testAccuracy)

      roc_score=roc_auc_score(Y_test, treemodel.predict_proba(X_test)[:, 1], multi_class='ovr')

      print("ROC_AUC_SCORE : ",roc_score)
     

      matrix1= confusion_matrix(Y_test, Y_testPred)
      #      sum of the total confusion matirx
      net_mat=net_mat+matrix1
     
      
      plot_confusion_matrix(matrix1,show_normed=True, colorbar=True, show_absolute=True)   
      
      plt.show()
      
      plt.figure(figsize=(15,10))
      tree.plot_tree(treemodel,feature_names=features.columns,filled=True)
      
    
  print("Result Of  Decisiom Tree : \n")
  avg_accuracy=(sum(acc) / len(acc))*100
  print("Average accuracy : ",avg_accuracy )     

  net_mat = net_mat.astype('int')
  print(net_mat)
  plot_confusion_matrix(net_mat,show_normed=True, colorbar=True, show_absolute=True, cmap='Blues')   

  plt.show()
   
  roc_score=roc_auc_score(Y, treemodel.predict_proba(X)[:, 1], multi_class='ovr')
  print("ROC_AUC_SCORE : ",roc_score)
 
  print("\nClassification Report : \n")
  report=classification_report(y_ori, y_pre,output_dict=True)
  report_Df=pd.DataFrame(report)
  print(report_Df)
  sns.heatmap(report_Df.T,annot=True)
  avg_accuracy

decisionTree()

"""# **Gaussian Naive Bayes Classifier**"""

from sklearn.naive_bayes import GaussianNB
def gaussianNaiveBayes():
  X=features.values
  Y=dependent.values
  # Check for NaN values and replace them with mean value of the corresponding column
  imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
  X = imp_mean.fit_transform(X)
  print("*************Normalizing the Featured/Independent Valuess*************")
  XScaled = normalize(X) 

  skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2)
  acc=[]

  y_ori = np.array([], dtype=int)

  y_pre= np.array([], dtype=int)
  net_mat=np.zeros((2,2))
  for train_index, test_index in skf.split(X,Y):
      
      X_train=X[train_index]
      X_test=X[test_index]
      Y_train=Y[train_index]
      Y_test=Y[test_index]

      print("\n") 
      print(f"Shape of X_train - {X_train.shape}  AND  Shape of X_test - {X_test.shape}")   
      print(f"Shape of Y_train - {Y_train.shape}  AND  Shape of Y_test - {Y_test.shape}")
      # print(f"For {i+1} fold : ")      
      print(f"\nTrain - {np.bincount(Y[train_index])}   AND  Test - {np.bincount(Y[test_index])} \n")

      GN = GaussianNB()
      GN.fit(X_train, Y_train)
      Y_testPred = GN.predict(X_test)
      
      y_ori=np.hstack((y_ori,Y_test))
      
  
      y_pre=np.hstack((y_pre,Y_testPred))
       
      testAccuracy = metrics.accuracy_score(Y_test, Y_testPred)
      print("Test Accuracy : ", testAccuracy*100)
      acc.append(testAccuracy)

      roc_score=roc_auc_score(Y_test, GN.predict_proba(X_test)[:, 1], multi_class='ovr')

      print("ROC_AUC_SCORE : ",roc_score)   

      matrix1= confusion_matrix(Y_test, Y_testPred)
      #      sum of the total confusion matirx
      net_mat=net_mat+matrix1
     
      
      plot_confusion_matrix(matrix1,show_normed=True, colorbar=True, show_absolute=True)   
      
      plt.show()
  print("Result Of Gaussian Naive Bayes Classifier : \n")
  print("Naive Bayes score : ",GN.score(X_test, Y_testPred))
  avg_accuracy= (sum(acc) / len(acc))*100
  print("Average accuracy : ", avg_accuracy)   

  net_mat = net_mat.astype('int')
  print(net_mat)
  plot_confusion_matrix(net_mat,show_normed=True, colorbar=True, show_absolute=True, cmap='Blues')   

  plt.show()
   
  roc_score=roc_auc_score(Y,GN.predict_proba(X)[:, 1], multi_class='ovr')
  print("ROC_AUC_SCORE : ",roc_score)
 
  
  
  print("\nClassification Report : \n")
  report=classification_report(y_ori, y_pre,output_dict=True)
  report_Df=pd.DataFrame(report)
  print(report_Df)
  sns.heatmap(report_Df.T,annot=True)
  avg_accuracy

gaussianNaiveBayes()

"""# **K- Nearest  Neighbour classifier**"""

from sklearn.neighbors import KNeighborsClassifier
def kNearestNeighbour():
  X=features.values
  Y=dependent.values
  # Check for NaN values and replace them with mean value of the corresponding column
  imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
  X = imp_mean.fit_transform(X)
  print("*************Normalizing the Featured/Independent Valuess*************")
  XScaled = normalize(X) 

  skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2)
  acc=[]

  y_ori = np.array([], dtype=int)

  y_pre= np.array([], dtype=int)
  net_mat=np.zeros((2,2))
  for train_index, test_index in skf.split(X,Y):
      
      X_train=X[train_index]
      X_test=X[test_index]
      Y_train=Y[train_index]
      Y_test=Y[test_index] 

      print("\n") 
      print(f"Shape of X_train - {X_train.shape}  AND  Shape of X_test - {X_test.shape}")   
      print(f"Shape of Y_train - {Y_train.shape}  AND  Shape of Y_test - {Y_test.shape}")
      # print(f"For {i+1} fold : ")      
      print(f"\nTrain - {np.bincount(Y[train_index])}   AND  Test - {np.bincount(Y[test_index])} \n")

      Kn = KNeighborsClassifier(n_neighbors=10)
      Kn.fit(X_train, Y_train)
      Y_testPred = Kn.predict(X_test)

      y_ori=np.hstack((y_ori,Y_test))
      
  
      y_pre=np.hstack((y_pre,Y_testPred))
       
      testAccuracy = metrics.accuracy_score(Y_test, Y_testPred)
      
      
      print("Test Accuracy : ", testAccuracy*100)
      acc.append(testAccuracy)

      roc_score=roc_auc_score(Y_test, Kn.predict_proba(X_test)[:, 1], multi_class='ovr')

      print("ROC_AUC_SCORE : ",roc_score)
    

      matrix1= confusion_matrix(Y_test, Y_testPred)
      # sum of the total confusion matirx
      net_mat=net_mat+matrix1
     
      
      plot_confusion_matrix(matrix1,show_normed=True, colorbar=True, show_absolute=True)   
      
      plt.show()
  avg_accuracy=(sum(acc) / len(acc))*100
  print("Average accuracy : ", avg_accuracy)      

  net_mat = net_mat.astype('int')
  print(net_mat)
  plot_confusion_matrix(net_mat,show_normed=True, colorbar=True, show_absolute=True, cmap='Blues')   

  plt.show()
   
  roc_score=roc_auc_score(Y,Kn.predict_proba(X)[:, 1], multi_class='ovr')
  print("ROC_AUC_SCORE : ",roc_score)
 
 
  
  print("\nClassification Report : \n")
  report=classification_report(y_ori, y_pre,output_dict=True)
  report_Df=pd.DataFrame(report)
  print(report_Df)
  sns.heatmap(report_Df.T,annot=True)
  avg_accuracy

kNearestNeighbour()